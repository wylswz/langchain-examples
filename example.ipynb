{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBED_MODEL = 'nomic-embed-text'\n",
    "LANG_MODEL = 'deepseek-r1:7b'\n",
    "LIB_ROOT = 'library'\n",
    "VECTOR_CACHE_ROOT = 'cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import os\n",
    "\n",
    "# knowledge base\n",
    "embedder = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "text_splitter = SemanticChunker(embedder)\n",
    "\n",
    "if not os.path.exists(VECTOR_CACHE_ROOT):\n",
    "    os.mkdir(VECTOR_CACHE_ROOT)\n",
    "\n",
    "# Retrieval setup\n",
    "def md5(filename):\n",
    "    import hashlib\n",
    "    import codecs\n",
    "\n",
    "    return hashlib.md5(codecs.encode(filename)).hexdigest()\n",
    "\n",
    "def to_doc(path):\n",
    "    from typing import List\n",
    "    from pydantic import TypeAdapter\n",
    "    adapter = TypeAdapter(List[Document])\n",
    "    ret: 'list[Document]'\n",
    "    cache_file = os.path.join(VECTOR_CACHE_ROOT, md5(path))\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'cache for {path} exists, will use cache')\n",
    "        with open(cache_file, 'rb') as c:\n",
    "            content = c.read()\n",
    "            return adapter.validate_json(content)\n",
    "\n",
    "    loader = PDFPlumberLoader(path)\n",
    "    print(f'loading doc {path}')\n",
    "    docs = loader.load()\n",
    "    print(f'spliting {path}')\n",
    "    ret = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        f.write(adapter.dump_json(ret))\n",
    "    return ret\n",
    "\n",
    "def build_vector_store():\n",
    "\n",
    "    docfiles = []\n",
    "    for file in os.listdir(LIB_ROOT):\n",
    "        docfiles.append(os.path.join(LIB_ROOT, file))\n",
    "    \n",
    "    docs = map(to_doc, docfiles)\n",
    "    documents = [d for ds in docs if ds is not None for d in ds]\n",
    "    print('building vector store')\n",
    "    vector = FAISS.from_documents(documents, embedder)\n",
    "\n",
    "    return vector\n",
    "\n",
    "vector = build_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee8f5a-6e25-46f1-a744-2c6c45b936e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "model = ChatOllama(model=LANG_MODEL, num_ctx=8192)\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Build graph\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def do_chat():\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"main\"\n",
    "    }\n",
    "}\n",
    "\n",
    "    input_message = input(\">: \")\n",
    "    # https://python.langchain.com/docs/concepts/streaming/\n",
    "    # for step in graph.stream(\n",
    "    #     {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    #     stream_mode=\"messages\",\n",
    "    #     config=config,\n",
    "    # ):\n",
    "    #     for chunk in step:\n",
    "    #         if isinstance(chunk, AIMessageChunk):\n",
    "    #             print(chunk.content, end='', flush=False)\n",
    "\n",
    "\n",
    "    for step in graph.stream(\n",
    "        {'question': input_message},\n",
    "        stream_mode=\"messages\", \n",
    "        config=config\n",
    "    ):\n",
    "        for chunk in step:\n",
    "            if isinstance(chunk, AIMessageChunk):\n",
    "                print(chunk.content, end='')\n",
    "\n",
    "while True:\n",
    "    do_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588401dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
