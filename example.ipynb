{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9c24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "import os\n",
    "\n",
    "EMBED_MODEL = 'nomic-embed-text'\n",
    "LANG_MODEL = 'qwen2.5:7b'\n",
    "LIB_ROOT = 'library'\n",
    "VECTOR_CACHE_ROOT = 'cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58a6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache for library\\infra-agent.pdf exists, will use cache\n",
      "cache for library\\mixture-of-experts.pdf exists, will use cache\n",
      "cache for library\\optimization.pdf exists, will use cache\n",
      "cache for library\\original.pdf exists, will use cache\n",
      "cache for library\\survey.pdf exists, will use cache\n",
      "building vector store\n"
     ]
    }
   ],
   "source": [
    "# knowledge base\n",
    "embedder = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "text_splitter = SemanticChunker(embedder)\n",
    "\n",
    "if not os.path.exists(VECTOR_CACHE_ROOT):\n",
    "    os.mkdir(VECTOR_CACHE_ROOT)\n",
    "\n",
    "# Retrieval setup\n",
    "def md5(filename):\n",
    "    import hashlib\n",
    "    import codecs\n",
    "\n",
    "    return hashlib.md5(codecs.encode(filename)).hexdigest()\n",
    "\n",
    "def to_doc(path):\n",
    "    from typing import List\n",
    "    from pydantic import TypeAdapter\n",
    "    adapter = TypeAdapter(List[Document])\n",
    "    ret: 'list[Document]'\n",
    "    cache_file = os.path.join(VECTOR_CACHE_ROOT, md5(path))\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'cache for {path} exists, will use cache')\n",
    "        with open(cache_file, 'rb') as c:\n",
    "            content = c.read()\n",
    "            return adapter.validate_json(content)\n",
    "\n",
    "    loader = PDFPlumberLoader(path)\n",
    "    print(f'loading doc {path}')\n",
    "    docs = loader.load()\n",
    "    print(f'spliting {path}')\n",
    "    ret = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        f.write(adapter.dump_json(ret))\n",
    "    return ret\n",
    "\n",
    "def build_vector_store():\n",
    "\n",
    "    docfiles = []\n",
    "    for file in os.listdir(LIB_ROOT):\n",
    "        docfiles.append(os.path.join(LIB_ROOT, file))\n",
    "    \n",
    "    docs = map(to_doc, docfiles)\n",
    "    documents = [d for ds in docs if ds is not None for d in ds]\n",
    "    print('building vector store')\n",
    "    vector = FAISS.from_documents(documents, embedder)\n",
    "\n",
    "    return vector\n",
    "\n",
    "vector = build_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3ee8f5a-6e25-46f1-a744-2c6c45b936e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('values', {'messages': [HumanMessage(content=\"what's proposed in OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS\", additional_kwargs={}, response_metadata={}, id='08e20e5c-f835-486b-91b7-019d9cb5f946')]})\n",
      "('values', {'messages': [HumanMessage(content=\"what's proposed in OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS\", additional_kwargs={}, response_metadata={}, id='08e20e5c-f835-486b-91b7-019d9cb5f946'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:7b', 'created_at': '2025-03-08T08:20:31.1641484Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11392279800, 'load_duration': 48665000, 'prompt_eval_count': 177, 'prompt_eval_duration': 623000000, 'eval_count': 70, 'eval_duration': 10715000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a2b3fa0c-f679-41d0-aaa9-8150fe8a48ec-0', tool_calls=[{'name': 'search_for_paper', 'args': {'query': 'OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS'}, 'id': 'b9f8e002-afd9-4187-b601-a83677c020cd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 70, 'total_tokens': 247})]})\n",
      "('values', {'messages': [HumanMessage(content=\"what's proposed in OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS\", additional_kwargs={}, response_metadata={}, id='08e20e5c-f835-486b-91b7-019d9cb5f946'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:7b', 'created_at': '2025-03-08T08:20:31.1641484Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11392279800, 'load_duration': 48665000, 'prompt_eval_count': 177, 'prompt_eval_duration': 623000000, 'eval_count': 70, 'eval_duration': 10715000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a2b3fa0c-f679-41d0-aaa9-8150fe8a48ec-0', tool_calls=[{'name': 'search_for_paper', 'args': {'query': 'OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS'}, 'id': 'b9f8e002-afd9-4187-b601-a83677c020cd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 70, 'total_tokens': 247}), ToolMessage(content=\"Source: {'source': 'library\\\\\\\\survey.pdf', 'file_path': 'library\\\\\\\\survey.pdf', 'page': 21, 'total_pages': 29, 'Author': '', 'CreationDate': 'D:20240809002726Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240809002726Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}\\nContent: Expert spe-\\nmodalities [158]. To address this limitation, Li et al. [158] cialization refers to the concept where each expert devel-\\nintroduce the pioneering Uni-MoE, a unified MLLM with opsnon-overlappingandfocusedknowledge.Encouraging\\nMoEarchitecturecapableofmanaginganextensiverangeof experts to concentrate their skills on distinct sub-tasks or\\nmodalities. They introduce a progressive training strategy domains has been shown to enhance the performance and\\nto bolster expert collaboration and generalization across generalization of the MoE model. The prevailing strategy\\nmodalities, and they utilize LoRA [174], a lightweight fine- involves designating a select number of experts as shared\\ntuningmethodology,tominimizecomputationaldemands. ones, with the goal of capturing commonalities in knowl-\\nedgeandreducingredundancyamongthoseexpertsthatare\\nrouteddynamically[38],[66],[69],[104].However,fostering\\n7 CHALLENGES & OPPORTUNITIES\\neffectivecollaborationamongthesespecializedexpertsisan\\nMixture of Experts (MoE) models present a compelling ongoing challenge. Relying solely on a sparsely computed\\napproach for significantly increasing model capacity at a weighted sum of outputs from the top-k experts can over-\\nconstant computational cost. Despite their promise, several look the intricate internal relationships that exist across the\\nintrinsic challenges remain, necessitating further collabo- entireexperts.Consequently,exploringnewmechanismsfor\\nrative design and engineering across algorithm, system, enhancing both the specialization and collaboration among\\nand application aspects. In this section, we identify critical experts is crucial for the development of more integrated\\nchallengesandpromisingdirectionsforfutureinvestigation andpowerfulMoEmodels. asfollows: Sparse Activation and Computational Efficiency. One\\nTraining Stability and Load Balancing. MoE models of the primary benefits of MoE models lies in their ca-\\nthat utilize sparse gating have become a popular means to pacity for sparse activations, which theoretically enhances\\nexpand model capacity without proportionally increasing computational efficiency. Nevertheless, implementing this\\ncomputational demands.\\n\\nSource: {'source': 'library\\\\\\\\mixture-of-experts.pdf', 'file_path': 'library\\\\\\\\mixture-of-experts.pdf', 'page': 10, 'total_pages': 13, 'Author': '', 'CreationDate': 'D:20240109022511Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240109022511Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}\\nContent: [35] YanqiZhou,TaoLei,HanxiaoLiu,NanDu,YanpingHuang,VincentZhao,AndrewMDai,\\nQuocVLe,JamesLaudon,etal. Mixture-of-expertswithexpertchoicerouting. Advancesin\\nNeuralInformationProcessingSystems,35:7103â€“7114,2022. 11\\n\", name='search_for_paper', id='f251c854-e3c8-4239-a469-99259dfc8008', tool_call_id='b9f8e002-afd9-4187-b601-a83677c020cd', artifact=[Document(id='d20e5755-0703-4be6-a7f9-7044c25ac017', metadata={'source': 'library\\\\survey.pdf', 'file_path': 'library\\\\survey.pdf', 'page': 21, 'total_pages': 29, 'Author': '', 'CreationDate': 'D:20240809002726Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240809002726Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Expert spe-\\nmodalities [158]. To address this limitation, Li et al. [158] cialization refers to the concept where each expert devel-\\nintroduce the pioneering Uni-MoE, a unified MLLM with opsnon-overlappingandfocusedknowledge.Encouraging\\nMoEarchitecturecapableofmanaginganextensiverangeof experts to concentrate their skills on distinct sub-tasks or\\nmodalities. They introduce a progressive training strategy domains has been shown to enhance the performance and\\nto bolster expert collaboration and generalization across generalization of the MoE model. The prevailing strategy\\nmodalities, and they utilize LoRA [174], a lightweight fine- involves designating a select number of experts as shared\\ntuningmethodology,tominimizecomputationaldemands. ones, with the goal of capturing commonalities in knowl-\\nedgeandreducingredundancyamongthoseexpertsthatare\\nrouteddynamically[38],[66],[69],[104].However,fostering\\n7 CHALLENGES & OPPORTUNITIES\\neffectivecollaborationamongthesespecializedexpertsisan\\nMixture of Experts (MoE) models present a compelling ongoing challenge. Relying solely on a sparsely computed\\napproach for significantly increasing model capacity at a weighted sum of outputs from the top-k experts can over-\\nconstant computational cost. Despite their promise, several look the intricate internal relationships that exist across the\\nintrinsic challenges remain, necessitating further collabo- entireexperts.Consequently,exploringnewmechanismsfor\\nrative design and engineering across algorithm, system, enhancing both the specialization and collaboration among\\nand application aspects. In this section, we identify critical experts is crucial for the development of more integrated\\nchallengesandpromisingdirectionsforfutureinvestigation andpowerfulMoEmodels. asfollows: Sparse Activation and Computational Efficiency. One\\nTraining Stability and Load Balancing. MoE models of the primary benefits of MoE models lies in their ca-\\nthat utilize sparse gating have become a popular means to pacity for sparse activations, which theoretically enhances\\nexpand model capacity without proportionally increasing computational efficiency. Nevertheless, implementing this\\ncomputational demands.'), Document(id='04b48c13-1b17-457d-8507-b1730c47fcbe', metadata={'source': 'library\\\\mixture-of-experts.pdf', 'file_path': 'library\\\\mixture-of-experts.pdf', 'page': 10, 'total_pages': 13, 'Author': '', 'CreationDate': 'D:20240109022511Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20240109022511Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[35] YanqiZhou,TaoLei,HanxiaoLiu,NanDu,YanpingHuang,VincentZhao,AndrewMDai,\\nQuocVLe,JamesLaudon,etal. Mixture-of-expertswithexpertchoicerouting. Advancesin\\nNeuralInformationProcessingSystems,35:7103â€“7114,2022. 11\\n')])]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: str\n",
    "    context: List[Document]\n",
    "    answer: AnswerWithSources\n",
    "\n",
    "# model and workflow\n",
    "@tool(response_format=\"content_and_artifact\", description=\"retrieve paper content from knowledgebase given keywords\")\n",
    "def search_for_paper(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "model = ChatOllama(model=LANG_MODEL, num_ctx=8192)\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: State):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    model_with_tool = model.bind_tools([search_for_paper])\n",
    "    response = model_with_tool.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([search_for_paper])\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "def do_chat():\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"main\"\n",
    "    }\n",
    "}\n",
    "\n",
    "    input_message = input(\">: \")\n",
    "    # https://python.langchain.com/docs/concepts/streaming/\n",
    "    # for step in graph.stream(\n",
    "    #     {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    #     stream_mode=\"messages\",\n",
    "    #     config=config,\n",
    "    # ):\n",
    "    #     for chunk in step:\n",
    "    #         if isinstance(chunk, AIMessageChunk):\n",
    "    #             print(chunk.content, end='', flush=False)\n",
    "\n",
    "\n",
    "    for step in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "        stream_mode=\"values\", config=config\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "while True:\n",
    "    do_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
