{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9c24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = 'nomic-embed-text'\n",
    "LANG_MODEL = 'gemma3:4b'\n",
    "SUMMARISE_MODEL = 'gemma3:1b'\n",
    "LIB_ROOT = 'library'\n",
    "VECTOR_CACHE_ROOT = 'cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d58a6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader, CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import os\n",
    "\n",
    "embedder = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "text_splitter = SemanticChunker(embedder)\n",
    "model = ChatOllama(model=LANG_MODEL, num_ctx=8192)\n",
    "summarizer = OllamaLLM(model=SUMMARISE_MODEL)\n",
    "\n",
    "if not os.path.exists(VECTOR_CACHE_ROOT):\n",
    "    os.mkdir(VECTOR_CACHE_ROOT)\n",
    "\n",
    "\n",
    "def md5(filename):\n",
    "    import hashlib\n",
    "    import codecs\n",
    "    return hashlib.md5(codecs.encode(filename)).hexdigest()\n",
    "\n",
    "def gen_text_and_table_summaries(path):\n",
    "    \"\"\"\n",
    "    从 PDF 中提取文本和表格, 使用大模型总结\n",
    "    \"\"\"\n",
    "    # Extract elements from PDF\n",
    "    def extract_pdf_elements(path):\n",
    "        \"\"\"\n",
    "        Extract images, tables, and chunk text from a PDF file.\n",
    "        path: File path, which is used to dump images (.jpg)\n",
    "        fname: File name\n",
    "        \"\"\"\n",
    "        return partition_pdf(\n",
    "            filename=path,\n",
    "            extract_images_in_pdf=False,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=4000,\n",
    "            new_after_n_chars=3800,\n",
    "            combine_text_under_n_chars=2000,\n",
    "            image_output_dir_path=path,\n",
    "            )\n",
    "\n",
    "\n",
    "    # Categorize elements by type\n",
    "    def categorize_elements(raw_pdf_elements):\n",
    "        \"\"\"\n",
    "        Categorize extracted elements from a PDF into tables and texts.\n",
    "        raw_pdf_elements: List of unstructured.documents.elements\n",
    "        \"\"\"\n",
    "        tables = []\n",
    "        texts = []\n",
    "        for element in raw_pdf_elements:\n",
    "            if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "                tables.append(str(element))\n",
    "            elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "                texts.append(str(element))\n",
    "        return texts, tables\n",
    "    \n",
    "    def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "        \"\"\"\n",
    "        Summarize text elements\n",
    "        texts: List of str\n",
    "        tables: List of str\n",
    "        summarize_texts: Bool to summarize texts\n",
    "        \"\"\"\n",
    "\n",
    "        # Prompt\n",
    "        prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "        These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "        Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "        # Text summary chain\n",
    "        summarize_chain = {\"element\": lambda x: x} | prompt | summarizer | StrOutputParser()\n",
    "\n",
    "        # Initialize empty summaries\n",
    "        text_summaries = []\n",
    "        table_summaries = []\n",
    "\n",
    "        # Apply to text if texts are provided and summarization is requested\n",
    "        if texts and summarize_texts:\n",
    "            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "        elif texts:\n",
    "            text_summaries = texts\n",
    "\n",
    "        # Apply to tables if tables are provided\n",
    "        if tables:\n",
    "            table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "        return text_summaries, table_summaries\n",
    "    raw = extract_pdf_elements(path)\n",
    "    texts, tables = categorize_elements(raw)\n",
    "    texts, tables = text_splitter.split_text(\" \".join(texts)), tables\n",
    "    return (texts, tables), generate_text_summaries(texts, tables, True)\n",
    "\n",
    "def get_img_summaries(image_path) -> tuple[list[str], list[str]]:\n",
    "    import base64\n",
    "    \"\"\"\n",
    "    summaries all images under dir\n",
    "    \"\"\"\n",
    "    def encode_image(path):\n",
    "        with open(path, 'rb') as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def image_summarize(img_base64, prompt):\n",
    "        \"\"\"Make image summary\"\"\"\n",
    "\n",
    "        msg = summarizer.invoke(\n",
    "            [\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return msg\n",
    "        # Store base64 encoded images\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "    # Apply to images\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "    return base64_image, image_summarize(base64_image, prompt)\n",
    "\n",
    "def to_doc(path: str):\n",
    "    \"\"\"\n",
    "    把给定文件转换成文档格式，主要是一个加载和分段的过程.\n",
    "    适用于简单 RAG\n",
    "    \"\"\"\n",
    "    from typing import List\n",
    "    from pydantic import TypeAdapter\n",
    "    adapter = TypeAdapter(List[Document])\n",
    "    ret: 'list[Document]'\n",
    "    cache_file = os.path.join(VECTOR_CACHE_ROOT, md5(path))\n",
    "    # 分段的过程比较慢（取决于 embedding 实现），因此我们用文件进行缓存\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'cache for {path} exists, will use cache')\n",
    "        with open(cache_file, 'rb') as c:\n",
    "            content = c.read()\n",
    "            return adapter.validate_json(content)\n",
    "    loader = None\n",
    "    if path.endswith('pdf'):\n",
    "        loader = PDFPlumberLoader(path)\n",
    "    if path.endswith('csv'):\n",
    "        loader = CSVLoader(path, encoding='utf-8')\n",
    "    print(f'loading doc {path}')\n",
    "    docs = loader.load()\n",
    "    print(f'spliting {path}')\n",
    "    ret = text_splitter.split_documents(docs)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        f.write(adapter.dump_json(ret))\n",
    "    return ret\n",
    "\n",
    "def build_vector_store():\n",
    "    \"\"\"\n",
    "    把所有 library 目录下的文档一起构建一个向量数据库\n",
    "    \"\"\"\n",
    "    docfiles = []\n",
    "    for file in os.listdir(LIB_ROOT):\n",
    "        docfiles.append(os.path.join(LIB_ROOT, file))\n",
    "    \n",
    "    docs = map(to_doc, docfiles)\n",
    "    documents = [d for ds in docs if ds is not None for d in ds]\n",
    "    print('building vector store')\n",
    "    vector = FAISS.from_documents(documents, embedder)\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def build_multi_modal_vector_store():\n",
    "    import uuid\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "    # Create the multi-vector retriever\n",
    "    # The vectorstore to use to index the summaries\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"mm_rag_cj_blog\", embedding_function=embedder\n",
    "    )\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "            doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "            summary_docs = [\n",
    "                Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "                for i, s in enumerate(doc_summaries)\n",
    "            ]\n",
    "            retriever.vectorstore.add_documents(summary_docs)\n",
    "            retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "    \n",
    "    for file in os.listdir(LIB_ROOT):\n",
    "        if file.endswith('pdf'):\n",
    "            (texts, tables ), (text_summaries, table_summaries) = gen_text_and_table_summaries(os.path.join(LIB_ROOT, file))\n",
    "\n",
    "            if text_summaries:\n",
    "                add_documents(retriever, text_summaries, texts)\n",
    "            if table_summaries:\n",
    "                add_documents(retriever, table_summaries, tables)\n",
    "        if file.endswith(('jpeg', 'png', 'jpg')):\n",
    "            img, img_summary = get_img_summaries(os.path.join(LIB_ROOT, file))\n",
    "            if img_summary:\n",
    "                add_documents(retriever,[img_summary], [img] )\n",
    "    return retriever\n",
    "\n",
    "# vector = build_vector_store()\n",
    "vector = build_multi_modal_vector_store()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adfe2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多模态 RAG\n",
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "\n",
    "def get_mime_type(img_b64):\n",
    "    MIME_TYPES = {\n",
    "        b\"\\xff\\xd8\\xff\": \"image/jpeg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"image/png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"image/gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"image/webp\",\n",
    "    }\n",
    "    head = base64.b64decode(img_b64)[:8]\n",
    "    for sig, format in MIME_TYPES.items():\n",
    "        if head.startswith(sig):\n",
    "            return format\n",
    "    return None\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sig = get_mime_type(b64data) # Decode and get the first 8 bytes\n",
    "        return sig is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:{get_mime_type(image)};base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    # 立人设的工作交给这个模板\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"\"\"\n",
    "\n",
    "here is the question:\n",
    "\n",
    "{formatted_texts}\n",
    "\"\"\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "chain_multimodal_rag = multi_modal_rag_chain(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47be37c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a breakdown and analysis of the provided text, focusing on key information and potential areas for further understanding.\n",
      "\n",
      "**Overall Summary:**\n",
      "\n",
      "The text describes the development of \"VisRAG,\" a novel Retrieval-Augmented Generation (RAG) pipeline designed to handle real-world, multi-modal documents (documents containing both text and images). It addresses the limitations of traditional RAG pipelines that rely solely on text-based retrieval, particularly when dealing with complex documents like textbooks and manuals. VisRAG leverages Vision-Language Models (VLMs) to directly process images within these documents, offering a more robust and accurate approach.\n",
      "\n",
      "**Key Components and Concepts:**\n",
      "\n",
      "* **Traditional RAG:** The text establishes the foundation by explaining the standard RAG pipeline: a retriever (typically a text encoder) and a generator (an LLM) working together to answer questions.\n",
      "* **Limitations of Traditional RAG:** The text highlights the challenges of applying traditional RAG to real-world documents, specifically the issue of hallucinations and the difficulty of updating knowledge bases.\n",
      "* **VisRAG Architecture:**\n",
      "    * **VisRAG-Ret:** The vision-based retriever, utilizing a VLM to map queries and documents into an embedding space, directly processing the document's image.\n",
      "    * **VisRAG-Gen:** The generator, leveraging open-source VLMs to produce answers based on the retrieved information.\n",
      "* **Multi-Modal Document Handling:** VisRAG's core innovation is its ability to handle documents with interleaved text and images, a common scenario in real-world applications.\n",
      "* **Data Construction:** The research team constructs datasets from VQA datasets and synthetic query-document pairs derived from web-crawled PDFs.\n",
      "* **Evaluation Metrics:** VisRAG is evaluated using MiniCPM-V 2.6 and GPT-40 as generators, demonstrating a significant improvement over TextRAG.\n",
      "* **Key Findings:**\n",
      "    * VisRAG exhibits superior performance in retrieving multi-modal documents.\n",
      "    * It outperforms text- and vision-centric retrievers.\n",
      "    * It demonstrates better training data efficiency and generalization ability.\n",
      "\n",
      "**Detailed Breakdown of Sections:**\n",
      "\n",
      "* **Section 2 (Related Work):** This section provides a comprehensive overview of existing RAG research, categorizing it into:\n",
      "    * **Improving the Retriever:** Focus on text encoders.\n",
      "    * **Enhancing the Generator:** Through fine-tuning, in-context learning, or prompting.\n",
      "    * **Advanced RAG Pipelines:** For long-form or multi-hop question answering.\n",
      "* **Section 3 (Methodology):** This section outlines the methodology, including:\n",
      "    * **Recap of RAG:**  A brief explanation of the standard RAG pipeline.\n",
      "    * **VisRAG Framework:**  Detailed description of the VisRAG architecture (VisRAG-Ret and VisRAG-Gen).\n",
      "    * **Data Construction:**  How the training and evaluation datasets were created.\n",
      "\n",
      "**Important Considerations and Questions Raised:**\n",
      "\n",
      "* **VLM Selection:** The text mentions using open-source VLMs like MiniCPM-V and GPT-40.  It would be beneficial to understand the specific VLMs chosen and the rationale behind their selection.\n",
      "* **Weighted Mean Pooling:** The use of weighted mean pooling in VisRAG-Ret is interesting.  The text doesn't fully explain the weighting scheme.\n",
      "* **Page Concatenation and Weighted Selection:**  The proposed techniques for handling multiple documents with VLMs that can only accept one image at a time are crucial.  More detail on these techniques would be valuable.\n",
      "* **Dataset Composition:**  Understanding the specifics of the VQA datasets and synthetic query-document pairs is important for assessing the generalizability of VisRAG.\n",
      "* **Future Work:** The text suggests potential for improved multi-page reasoning with VLMs capable of handling multiple images.\n",
      "\n",
      "**Overall, this is a well-structured and informative description of a promising research direction. The focus on real-world multi-modal document understanding is a significant advancement in the field of RAG.**\n",
      "\n",
      "Do you want me to:\n",
      "\n",
      "*   Expand on a specific aspect of the text?\n",
      "*   Generate questions based on the text?\n",
      "*   Help you summarize a particular section?\n"
     ]
    }
   ],
   "source": [
    "docs = chain_multimodal_rag.invoke(\"\"\"\n",
    "                                   \"\"\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ee8f5a-6e25-46f1-a744-2c6c45b936e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    189\u001b[39m memory = MemorySaver()\n\u001b[32m    190\u001b[39m graph = graph_builder.compile(checkpointer=memory)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m chain_multimodal_rag = \u001b[43mmulti_modal_rag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream_chat\u001b[39m():\n\u001b[32m    195\u001b[39m     config = {\n\u001b[32m    196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m     }\n\u001b[32m    199\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mmulti_modal_rag_chain\u001b[39m\u001b[34m(retriever)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03mMulti-modal RAG chain\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# RAG pipeline\u001b[39;00m\n\u001b[32m    140\u001b[39m chain = (\n\u001b[32m    141\u001b[39m     {\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mretriever\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunnableLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_image_text_types\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    143\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough(),\n\u001b[32m    144\u001b[39m     }\n\u001b[32m    145\u001b[39m     | RunnableLambda(img_prompt_func)\n\u001b[32m    146\u001b[39m     | model\n\u001b[32m    147\u001b[39m     | StrOutputParser()\n\u001b[32m    148\u001b[39m )\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chain\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\makaveli\\Desktop\\workspace\\rag-test\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:587\u001b[39m, in \u001b[36mRunnable.__ror__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ror__\u001b[39m(\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    579\u001b[39m     other: Union[\n\u001b[32m   (...)\u001b[39m\u001b[32m    584\u001b[39m     ],\n\u001b[32m    585\u001b[39m ) -> RunnableSerializable[Other, Output]:\n\u001b[32m    586\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\makaveli\\Desktop\\workspace\\rag-test\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5853\u001b[39m, in \u001b[36mcoerce_to_runnable\u001b[39m\u001b[34m(thing)\u001b[39m\n\u001b[32m   5848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5849\u001b[39m     msg = (\n\u001b[32m   5850\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Runnable, callable or dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5851\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5852\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5853\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[31mTypeError\u001b[39m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\n",
    "# 普通问答 RAG\n",
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"\n",
    "    带有数据源引用的文档输出格式，让生成的内容更有依据。\n",
    "    \"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    这是 langchain 图模型中流转的状态对象。我们增加 context 来保存上下文\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 提示词模板，我们让它不要不懂装懂，以检索到的信息为准\n",
    "# 并且言简意赅\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    信息检索，从向量库进行搜索\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"\n",
    "    生成内容。把 context 进行拼接，生成提示词给到模型\n",
    "    \"\"\"\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 构建图模型\n",
    "# 总是先检索，再生成\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def stream_chat():\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"main\"\n",
    "    }\n",
    "}\n",
    "\n",
    "    input_message = input(\">: \")\n",
    "    # https://python.langchain.com/docs/concepts/streaming/\n",
    "    # for step in graph.stream(\n",
    "    #     {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    #     stream_mode=\"messages\",\n",
    "    #     config=config,\n",
    "    # ):\n",
    "    #     for chunk in step:\n",
    "    #         if isinstance(chunk, AIMessageChunk):\n",
    "    #             print(chunk.content, end='', flush=False)\n",
    "\n",
    "    print(\"\\n============ User Message ==========\")\n",
    "    print(input_message)\n",
    "\n",
    "    for step in graph.stream(\n",
    "        {'question': input_message},\n",
    "        stream_mode=\"messages\", \n",
    "        config=config\n",
    "    ):\n",
    "        # 使用 message 类型的 stream 来实现逐字生成的效果。\n",
    "        for chunk in step:\n",
    "            if isinstance(chunk, AIMessageChunk):\n",
    "                print(chunk.content, end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588401dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
