{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from common import get_vector_store\n",
    "\n",
    "\n",
    "initial_proppt = \"how to computationally design an antibody for H5N1\"\n",
    "vector = get_vector_store(\"collection2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化前\n",
    "res = vector.as_retriever().invoke(initial_proppt)\n",
    "\n",
    "for d in res:\n",
    "    print(d.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "llm = ChatOllama(model=\"llama3.1:latest\")\n",
    "\n",
    "tmpl = \"\"\"\n",
    "You are a Query Optimization Engine that takes a user query and a context and returns a list of optimized search queries.\n",
    "\n",
    "The original query is used to generate a plan, but is not quite suitable for retrieval, \n",
    "\n",
    "try to enrich the user query with provided professional knowledge and tools in the context.\n",
    "\n",
    "Make sure the returned queries are precise and have low noise.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Original Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(tmpl)\n",
    "\n",
    "class Output(BaseModel):\n",
    "   queries: list[str] = Field(description=\"Optimized search queries\")\n",
    "\n",
    "chain = prompt | llm.with_structured_output(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = \"\"\"\n",
    "You have tools available:\n",
    "1. Alphafold3, Foldx: predict the 3D structure of antibody sequences from an input Excel file and saves the result as a PDB file.\n",
    "2. Gearbind, ddg: predicts binding affinities of antibodies\n",
    "\"\"\"\n",
    "\n",
    "output: Output = chain.invoke({\n",
    "    \"query\": initial_proppt,\n",
    "    \"context\": context\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "print(f\"Initial prompt: {initial_proppt}\\n\")\n",
    "\n",
    "print(f\"Optimized query for RAG: {output.queries}\\n\")\n",
    "\n",
    "print(\"Retrieved documents: \\n\")\n",
    "for q in output.queries:\n",
    "    res: list[Document] = vector.as_retriever().invoke(q)\n",
    "    for d in res:\n",
    "        print(f'{d.metadata['source']}, page: {d.metadata[\"page\"]}')\n",
    "        print(\"------content------\")\n",
    "        print(d.page_content)\n",
    "        print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
